{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Inferencing Service в Azure ML\n",
    "\n",
    "Цель лабораторной работы: \n",
    "\n",
    "- развертывание обученнной ML модели, как __web-сервиса, работающего в пакетном режиме__ (Batch Inferencing Service)\n",
    "- подключение и получение прогнозов от web-сервиса, работающего в пакетном режиме.\n",
    "\n",
    "\n",
    "В предыдущих лабораторных работах мы использовали Конвейер машинного обучения для автоматизации обучения, а также web-сервисы Azure ML для вывода в режиме реального времени результатов прогноза модели.\n",
    "В рамках этой лабораторной работы мы объединим эти концепции и создадим Конвейер машинного обучения для пакетного вывода результатов прогнозов модели.\n",
    "\n",
    "Представьте себе, что медицинская клиника проводит измерения пациентов в течение всего дня, сохраняя детали для каждого пациента в отдельном файле. Затем в течение ночи модель машинного обучения обрабатывает собранные данные и делает прогнозы по пациентам, которые подвержены риску развития диабета, чтобы с утра врачи клиники уже имели нужные данные.\n",
    "Давайте реализуем описанную функциональность в данной лабораторной работе.\n",
    "\n",
    "\n",
    "## Подготовка среды\n",
    "\n",
    "Импорт необходимых модулей и проверка версии AzureML SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Model, Environment, Experiment, ComputeTarget, Datastore, Dataset\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# Check core SDK version number\n",
    "print(f'SDK version: {azureml.core.VERSION}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим параметры Эксперимента:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'batch_service_demo'\n",
    "\n",
    "experiment_dir = 'batch-service-demo'\n",
    "os.makedirs(experiment_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Соединение со Azure ML Workspace\n",
    "\n",
    "Устанавливаем соединение с Рабочей областью в Azure ML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print(f'Successfully connected to Workspace: {ws.name}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка к развертыванию сервиса пакетной обработки\n",
    "\n",
    "### Получение ML модели\n",
    "\n",
    "Получим список уже обученных и зарегистрированных в Azure ML моделей машинного обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in Model.list(ws):\n",
    "    print(f'{model.name} v{model.version}')\n",
    "    \n",
    "    for tag_name in model.tags:\n",
    "        tag = model.tags[tag_name]\n",
    "        print(f'\\t {tag_name}: {tag}')\n",
    "        \n",
    "    for prop_name in model.properties:\n",
    "        prop = model.properties[prop_name]\n",
    "        print(f'\\t {prop}: {prop_name}')\n",
    "        \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выберем ML модель, которую хотим развернуть как web-сервис (по умолчанию берется последняя версия модели):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ws.models['diabetes_predict_model']\n",
    "print(f'{model.name} v{model.version}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Скрипт для прогнозов\n",
    "\n",
    "Создадим скрипт, который содержит методы необходимые для:\n",
    "\n",
    "- `init()`: инициализации web-сервиса\n",
    "- `run(mini_batch)`: прогноза на пакетах с новыми данными.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/score_model.py\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "from azureml.core.model import Model\n",
    "\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    # AZUREML_MODEL_DIR is an environment variable created during deployment. Join this path with the filename of the model file.\n",
    "    # It holds the path to the directory that contains the deployed model (./azureml-models/$MODEL_NAME/$VERSION).\n",
    "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'model.pkl')\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "\n",
    "def run(mini_batch):\n",
    "    # This runs for each batch\n",
    "    results = []\n",
    "\n",
    "    # process each file in the batch\n",
    "    for f in mini_batch:\n",
    "        # Read the comma-delimited data into an array\n",
    "        data = np.genfromtxt(f, delimiter=',')\n",
    "        # Reshape into a 2-dimensional array for prediction (model expects multiple items)\n",
    "        prediction = model.predict(data.reshape(1, -1))\n",
    "        # Append prediction to results\n",
    "        results.append(\"{}: {}\".format(os.path.basename(f), prediction[0]))\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скопируем скрипт в директорию эксперимента:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp scripts/score_model.py $experiment_dir\n",
    "!ls $experiment_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Среда для запуска сервиса пакетной обработки\n",
    "\n",
    "Создадим Среду с необходимыми зависимостями:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.core.runconfig import CondaDependencies\n",
    "\n",
    "# Add dependencies required by the model\n",
    "# - scikit-learn you need for trained model\n",
    "# - azureml-core and azureml-dataprep[fuse] ypu need to parallel pipeline steps\n",
    "dependencies = CondaDependencies.create(pip_packages=['scikit-learn','azureml-core','azureml-defaults','azureml-dataprep[fuse]'])\n",
    "\n",
    "batch_env = Environment(name='batch_environment')\n",
    "batch_env.python.conda_dependencies = dependencies\n",
    "batch_env.docker.enabled = True\n",
    "batch_env.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "\n",
    "print('Configuration is ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML кластер для запуска сервиса пакетной обработки\n",
    "\n",
    "Используем [ранее созданный](05B-compute-targets.ipynb) ML кластер для нашей задачи. Для этого получим список созданных Целевых объектов вычислений, а затем выберем нужный:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ct in ComputeTarget.list(ws):\n",
    "    print(ct.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_name = 'ml-cluster'\n",
    "cluster = ComputeTarget(workspace=ws, name=cluster_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Данные \n",
    "\n",
    "Сгенерируем множество пакетов из [зарегистрованного в Azure ML ранее](03-datastores-and-datasets.ipynb) Набора данных. Для этого получим список зарегистрированных Наборов данных, а затем выберем нужный:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in Dataset.get_all(ws):\n",
    "    print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ds = ws.datasets.get('diabetes_db')\n",
    "print(f'Used dataset {data_ds.name}: {data_ds.description}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерируем из полученного на предыдущем шаге набора данных множество пакетов данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Generate data sample\n",
    "data_df = data_ds.to_pandas_dataframe()\n",
    "data_col = list(set(data_df.columns) - set(['PatientID', 'Diabetic'])) # remove not features (PatientID) and label (Diabetic) \n",
    "\n",
    "n_batches = 128 # Set number of batches\n",
    "data_sample = data_df[data_col].sample(n=n_batches).values\n",
    "\n",
    "\n",
    "#%% Create a input dir for store batches\n",
    "batch_dir = f'{experiment_dir}/input'\n",
    "os.makedirs(batch_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "#%% Save each sample as a separate file\n",
    "print('Saving files..')\n",
    "\n",
    "for i in range(n_batches):\n",
    "    path = os.path.join(batch_dir, str(i+1) + '.csv')\n",
    "    data_sample[i].tofile(path, sep=\",\")\n",
    "        \n",
    "print('All files were saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Upload the files to the default datastore\n",
    "print('Uploading files to datastore...')\n",
    "\n",
    "default_ds = ws.get_default_datastore()\n",
    "default_ds.upload(src_dir=batch_dir, target_path='diabetes-batch-data', overwrite=True, show_progress=True)\n",
    "\n",
    "\n",
    "#%% Register a dataset for the input data\n",
    "batch_data_set = Dataset.File.from_files(path=(default_ds, 'diabetes-batch-data/'), validate=False)\n",
    "try:\n",
    "    batch_data_set = batch_data_set.register(workspace=ws, \n",
    "                                             name='diabetes-batch-data',\n",
    "                                             description='Diabetes data for Batch Inferencing Service',\n",
    "                                             create_new_version=True)\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "\n",
    "print('All files were uploaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание и запуск ML Конвейера\n",
    "\n",
    "Чтобы использовать ML Конвейер для запуска сценария пакетного прогнозирования, необходимо использовать `ParallelRunStep`, который позволяет обрабатывать пакетные данные параллельно и записывать результаты в одном выходном файле с именем `parallel_run_step.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import ParallelRunConfig, ParallelRunStep\n",
    "\n",
    "\n",
    "pipeline_output_dir = 'diabetes/outputs'\n",
    "output_dir = PipelineData(name='batch_inferences_pipe', \n",
    "                          datastore=ws.get_default_datastore(), \n",
    "                          output_path_on_compute=pipeline_output_dir)\n",
    "\n",
    "\n",
    "run_config = ParallelRunConfig(\n",
    "    source_directory=experiment_dir,\n",
    "    entry_script='score_model.py',\n",
    "    mini_batch_size='8',\n",
    "    error_threshold=2,\n",
    "    output_action='append_row',\n",
    "    environment=batch_env,\n",
    "    compute_target=cluster,\n",
    "    node_count=2\n",
    ")\n",
    "\n",
    "batch_score_step = ParallelRunStep(\n",
    "    name='batch-score',\n",
    "    parallel_run_config=run_config,\n",
    "    inputs=[batch_data_set.as_named_input('data')],\n",
    "    output=output_dir,\n",
    "    arguments=[],\n",
    "    allow_reuse=True\n",
    ")\n",
    "\n",
    "print('Steps were defined.')\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[batch_score_step])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим Конвейер:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run = Experiment(ws, experiment_name).submit(pipeline)\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Публикация ML Конвейера как REST-сервиса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = pipeline_run.publish_pipeline(\n",
    "    name='diabetes_batch_pipeline', \n",
    "    description='Batch Scoring Pipeline of Diabetes Data', version='1.0')\n",
    "\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим URI Конечной точки созданного REST-сервиса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = pipeline.endpoint\n",
    "print(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подключение и взаимодействие с REST-сервисом\n",
    "\n",
    "Сформируем авторизационный заголовок для отправки запроса к сервису:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "interactive_auth = InteractiveLoginAuthentication()\n",
    "auth_header = interactive_auth.get_authentication_header()\n",
    "\n",
    "print('Authentication is header ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отправим запрос к сервису и получим id запроса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.post(endpoint, \n",
    "                         headers=auth_header, \n",
    "                         json={'ExperimentName': experiment_name})\n",
    "\n",
    "run_id = response.json()['Id']\n",
    "run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим детали запуска во аремя выполнения Эксперимента:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core.run import PipelineRun\n",
    "\n",
    "pipeline_run = PipelineRun(ws.experiments[experiment_name], run_id)\n",
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда Конвейер завершит работу, прогнозы ML модели будут сохранены в выходных данных эксперимента. Вы можете получить их следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "\n",
    "shutil.rmtree(pipeline_output_dir, ignore_errors=True)\n",
    "\n",
    "prediction_run = next(published_pipeline_run.get_children())\n",
    "prediction_output = prediction_run.get_output_data(output_dir.name)\n",
    "prediction_output.download(local_path=pipeline_output_dir)\n",
    "\n",
    "\n",
    "for root, dirs, files in os.walk(pipeline_output_dir):\n",
    "    for file in files:\n",
    "        if file.endswith('parallel_run_step.txt'):\n",
    "            result_file = os.path.join(root,file)\n",
    "\n",
    "# cleanup output format\n",
    "df = pd.read_csv(result_file, delimiter=':', header=None)\n",
    "df.columns = ['File', 'Prediction']\n",
    "\n",
    "# Display the first 20 results\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вывод\n",
    "\n",
    "## Полезные ссылки\n",
    "\n",
    "1. https://docs.microsoft.com/ru-ru/azure/machine-learning/how-to-deploy-and-where?tabs=azcli\n",
    "2. https://docs.microsoft.com/ru-ru/azure/machine-learning/how-to-deploy-and-where?tabs=azcli#choose-a-compute-target "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
